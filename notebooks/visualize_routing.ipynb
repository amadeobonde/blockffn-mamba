{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlockFFN + Mamba Hybrid: Routing Signal Visualization\n",
    "\n",
    "This notebook visualizes the routing signals extracted from BlockFFN to understand:\n",
    "1. How routing sparsity varies across tokens\n",
    "2. Whether sparsity correlates with semantic importance\n",
    "3. How routing patterns differ across layers\n",
    "\n",
    "Run `scripts/03_extract_routing.py` first to generate the routing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from config import OUTPUT_DIR, ROUTING_SIGNALS_PATH, ALPHA_SWEEP_RESULTS_PATH, GATED_HYBRID_RESULTS_PATH\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Routing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load routing signals from JSON\n",
    "routing_path = project_root / ROUTING_SIGNALS_PATH\n",
    "\n",
    "if routing_path.exists():\n",
    "    with open(routing_path) as f:\n",
    "        routing_data = json.load(f)\n",
    "    print(f\"Loaded routing data from {routing_path}\")\n",
    "    print(f\"Number of prompts: {len(routing_data.get('prompts', []))}\")\n",
    "else:\n",
    "    print(f\"Routing data not found at {routing_path}\")\n",
    "    print(\"Run scripts/03_extract_routing.py first!\")\n",
    "    routing_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Sparsity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if routing_data:\n",
    "    prompts_data = routing_data.get('prompts', [])\n",
    "    \n",
    "    # Extract overall sparsity for each prompt\n",
    "    sparsities = [p['overall_sparsity'] for p in prompts_data]\n",
    "    prompts = [p['prompt'][:50] + '...' if len(p['prompt']) > 50 else p['prompt'] for p in prompts_data]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    bars = ax.barh(range(len(sparsities)), sparsities, color='steelblue')\n",
    "    ax.set_yticks(range(len(prompts)))\n",
    "    ax.set_yticklabels(prompts, fontsize=9)\n",
    "    ax.set_xlabel('Sparsity (fraction of zeros)', fontsize=12)\n",
    "    ax.set_title('Overall Routing Sparsity by Prompt', fontsize=14)\n",
    "    ax.axvline(x=0.7, color='red', linestyle='--', label='High sparsity threshold (0.7)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, sparsities)):\n",
    "        ax.text(val + 0.01, i, f'{val:.2f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nMean sparsity: {np.mean(sparsities):.3f}\")\n",
    "    print(f\"Std sparsity: {np.std(sparsities):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Per-Layer Sparsity Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if routing_data:\n",
    "    prompts_data = routing_data.get('prompts', [])\n",
    "    \n",
    "    # Build matrix of per-layer sparsity\n",
    "    all_layers = set()\n",
    "    for p in prompts_data:\n",
    "        all_layers.update(p.get('per_layer_sparsity', {}).keys())\n",
    "    \n",
    "    layers = sorted([int(l) for l in all_layers])\n",
    "    \n",
    "    if layers:\n",
    "        sparsity_matrix = np.zeros((len(prompts_data), len(layers)))\n",
    "        \n",
    "        for i, p in enumerate(prompts_data):\n",
    "            layer_sparsity = p.get('per_layer_sparsity', {})\n",
    "            for j, layer in enumerate(layers):\n",
    "                sparsity_matrix[i, j] = layer_sparsity.get(str(layer), 0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        im = ax.imshow(sparsity_matrix, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "        \n",
    "        # Show every 5th layer on x-axis for readability\n",
    "        xtick_indices = range(0, len(layers), max(1, len(layers)//10))\n",
    "        ax.set_xticks(list(xtick_indices))\n",
    "        ax.set_xticklabels([layers[i] for i in xtick_indices])\n",
    "        \n",
    "        ax.set_yticks(range(len(prompts_data)))\n",
    "        ax.set_yticklabels([p['prompt'][:30] + '...' for p in prompts_data], fontsize=9)\n",
    "        \n",
    "        ax.set_xlabel('Layer', fontsize=12)\n",
    "        ax.set_ylabel('Prompt', fontsize=12)\n",
    "        ax.set_title('Routing Sparsity Across Layers and Prompts', fontsize=14)\n",
    "        \n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Sparsity', fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Average sparsity per layer\n",
    "        avg_per_layer = sparsity_matrix.mean(axis=0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        ax.plot(layers, avg_per_layer, 'o-', markersize=4)\n",
    "        ax.fill_between(layers, avg_per_layer, alpha=0.3)\n",
    "        ax.set_xlabel('Layer', fontsize=12)\n",
    "        ax.set_ylabel('Average Sparsity', fontsize=12)\n",
    "        ax.set_title('Average Routing Sparsity by Layer', fontsize=14)\n",
    "        ax.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='Threshold (0.7)')\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Token-Level Analysis\n",
    "\n",
    "Examine how sparsity varies across tokens within a prompt. Do certain token types (punctuation, function words) consistently show higher sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if routing_data and prompts_data:\n",
    "    # Use first prompt for detailed analysis\n",
    "    example = prompts_data[0]\n",
    "    tokens = example.get('tokens', [])\n",
    "    \n",
    "    print(f\"Analyzing prompt: {example['prompt']!r}\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "    print(f\"\\nTokens: {tokens}\")\n",
    "    \n",
    "    # Note: We don't have per-token sparsity saved in JSON (too large)\n",
    "    # This would require running the extractor again or saving more data\n",
    "    print(\"\\nNote: Per-token sparsity visualization requires running the model.\")\n",
    "    print(\"To see per-token sparsity, modify 03_extract_routing.py to save per-token data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alpha Sweep Results\n",
    "\n",
    "Visualize how perplexity changes with different alpha values (Mamba contribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_path = project_root / ALPHA_SWEEP_RESULTS_PATH\n",
    "\n",
    "if alpha_path.exists():\n",
    "    alpha_df = pd.read_csv(alpha_path)\n",
    "    print(f\"Loaded alpha sweep results from {alpha_path}\")\n",
    "    display(alpha_df)\n",
    "    \n",
    "    # Filter to numeric alpha values\n",
    "    numeric_df = alpha_df[alpha_df['alpha'].apply(lambda x: isinstance(x, (int, float)) or (isinstance(x, str) and x.replace('.','').isdigit()))].copy()\n",
    "    numeric_df['alpha'] = pd.to_numeric(numeric_df['alpha'])\n",
    "    numeric_df = numeric_df.sort_values('alpha')\n",
    "    \n",
    "    if not numeric_df.empty and 'perplexity' in numeric_df.columns:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Perplexity vs Alpha\n",
    "        ax1 = axes[0]\n",
    "        ax1.plot(numeric_df['alpha'], numeric_df['perplexity'], 'o-', markersize=8, linewidth=2)\n",
    "        ax1.set_xlabel('Alpha (0=Attention, 1=Mamba)', fontsize=12)\n",
    "        ax1.set_ylabel('Perplexity', fontsize=12)\n",
    "        ax1.set_title('Perplexity vs Alpha', fontsize=14)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add baseline reference\n",
    "        baseline_row = alpha_df[alpha_df['alpha'] == 'baseline']\n",
    "        if not baseline_row.empty:\n",
    "            baseline_ppl = baseline_row['perplexity'].values[0]\n",
    "            ax1.axhline(y=baseline_ppl, color='red', linestyle='--', label=f'Baseline: {baseline_ppl:.2f}')\n",
    "            ax1.legend()\n",
    "        \n",
    "        # Throughput vs Alpha (if available)\n",
    "        ax2 = axes[1]\n",
    "        if 'tokens_per_second' in numeric_df.columns:\n",
    "            ax2.plot(numeric_df['alpha'], numeric_df['tokens_per_second'], 'o-', markersize=8, linewidth=2, color='green')\n",
    "            ax2.set_xlabel('Alpha', fontsize=12)\n",
    "            ax2.set_ylabel('Tokens/Second', fontsize=12)\n",
    "            ax2.set_title('Throughput vs Alpha', fontsize=14)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'Throughput data not available', ha='center', va='center', transform=ax2.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Alpha sweep results not found at {alpha_path}\")\n",
    "    print(\"Run scripts/05_sweep_alpha.py first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gated Hybrid Results\n",
    "\n",
    "Compare routing-based gating vs fixed alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_path = project_root / GATED_HYBRID_RESULTS_PATH\n",
    "\n",
    "if gated_path.exists():\n",
    "    gated_df = pd.read_csv(gated_path)\n",
    "    print(f\"Loaded gated hybrid results from {gated_path}\")\n",
    "    display(gated_df)\n",
    "    \n",
    "    if 'perplexity' in gated_df.columns:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        colors = []\n",
    "        for mode in gated_df['mode']:\n",
    "            if 'routing' in mode:\n",
    "                colors.append('steelblue')\n",
    "            elif 'fixed' in mode:\n",
    "                colors.append('orange')\n",
    "            else:\n",
    "                colors.append('gray')\n",
    "        \n",
    "        bars = ax.barh(range(len(gated_df)), gated_df['perplexity'], color=colors)\n",
    "        ax.set_yticks(range(len(gated_df)))\n",
    "        ax.set_yticklabels(gated_df['mode'])\n",
    "        ax.set_xlabel('Perplexity', fontsize=12)\n",
    "        ax.set_title('Perplexity by Gating Mode', fontsize=14)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, gated_df['perplexity'])):\n",
    "            ax.text(val + 0.1, i, f'{val:.2f}', va='center', fontsize=9)\n",
    "        \n",
    "        # Legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='steelblue', label='Routing-based'),\n",
    "            Patch(facecolor='orange', label='Fixed alpha'),\n",
    "            Patch(facecolor='gray', label='Baseline')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='lower right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find best in each category\n",
    "        routing_results = gated_df[gated_df['mode'].str.contains('routing', case=False)]\n",
    "        fixed_results = gated_df[gated_df['mode'].str.contains('fixed', case=False)]\n",
    "        \n",
    "        if not routing_results.empty:\n",
    "            best_routing = routing_results.loc[routing_results['perplexity'].idxmin()]\n",
    "            print(f\"\\nBest routing-based: {best_routing['mode']} (perplexity: {best_routing['perplexity']:.2f})\")\n",
    "        \n",
    "        if not fixed_results.empty:\n",
    "            best_fixed = fixed_results.loc[fixed_results['perplexity'].idxmin()]\n",
    "            print(f\"Best fixed alpha: {best_fixed['mode']} (perplexity: {best_fixed['perplexity']:.2f})\")\n",
    "else:\n",
    "    print(f\"Gated hybrid results not found at {gated_path}\")\n",
    "    print(\"Run scripts/07_gated_hybrid.py first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if routing_data:\n",
    "    mean_sparsity = np.mean([p['overall_sparsity'] for p in routing_data.get('prompts', [])])\n",
    "    print(f\"\\n1. Routing Sparsity: {mean_sparsity:.1%}\")\n",
    "    if mean_sparsity > 0.7:\n",
    "        print(\"   -> HIGH: Most tokens activate few experts (good for Mamba)\")\n",
    "    else:\n",
    "        print(\"   -> MODERATE: Mixed importance across tokens\")\n",
    "\n",
    "if alpha_path.exists():\n",
    "    alpha_df = pd.read_csv(alpha_path)\n",
    "    print(f\"\\n2. Alpha Sweep:\")\n",
    "    baseline = alpha_df[alpha_df['alpha'] == 'baseline']\n",
    "    if not baseline.empty:\n",
    "        print(f\"   Baseline perplexity: {baseline['perplexity'].values[0]:.2f}\")\n",
    "\n",
    "if gated_path.exists():\n",
    "    gated_df = pd.read_csv(gated_path)\n",
    "    print(f\"\\n3. Gated Hybrid:\")\n",
    "    \n",
    "    routing_results = gated_df[gated_df['mode'].str.contains('routing', case=False)]\n",
    "    fixed_results = gated_df[gated_df['mode'].str.contains('fixed', case=False)]\n",
    "    \n",
    "    if not routing_results.empty and not fixed_results.empty:\n",
    "        best_routing_ppl = routing_results['perplexity'].min()\n",
    "        best_fixed_ppl = fixed_results['perplexity'].min()\n",
    "        \n",
    "        print(f\"   Best routing-based perplexity: {best_routing_ppl:.2f}\")\n",
    "        print(f\"   Best fixed-alpha perplexity: {best_fixed_ppl:.2f}\")\n",
    "        \n",
    "        if best_routing_ppl < best_fixed_ppl:\n",
    "            print(\"\\n   CONCLUSION: Routing-based gating outperforms fixed alpha!\")\n",
    "            print(\"   The hypothesis is SUPPORTED.\")\n",
    "        else:\n",
    "            print(\"\\n   CONCLUSION: Fixed alpha performs better.\")\n",
    "            print(\"   Further investigation needed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
